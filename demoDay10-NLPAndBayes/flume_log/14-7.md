# 数据采集业务+flume
## 一、数据采集业务
- 整体架构：
![ac9cbcae5d0c65bb37499183330e0177.png](en-resource://database/4290:1)

>画图软件：迅捷
同学推荐：xmind，FreeMind

- 推荐系统结合详情页请求：
![e60c640850dae04d50ea3fa0bf471a25.png](en-resource://database/4292:1)

- 为什么前端采集数据成本很高，主要是在app上埋点增加需要重新发布app版本
![473e5a2ab7e97284030ea3631cfbcb51.png](en-resource://database/4294:1)

### 关于行为数据和数据库中的数据采集
 1. 埋点采集用户实时行为数据到hdfs，SDK将日志打到Kafka，日志是通过flume从不同的服务器中收集到Kafka 
 2. 从线上业务数据库（oracal mysql）到hdfs。
    2.1. **全量采集：sqoop**
 优点：所有数据和线上业务数据一致
 缺点：拉取数据量过大，时间比较集中，会给线上业务数据库带来性能影响，是的已有数据任务带来问题。
    2.2. **增量采集**
 第一次需要用到全量的数据，但是拉取时间最好在凌晨业务量最小的时间段进行。
 后续进行增量：
 当业务数据（增删改）操作日志打到日志文件，改或者删的数据进行操作之后，数据打到日志文件中，再通过flume写到hdfs。

日志到kafka流程了解：
![cb6c1cde44cd393d8bc169b1ed576c54.png](en-resource://database/4296:1)
Canal：基于数据库增量日志解析，提供增量数据的订阅&消费（Mysql）
GoldenGate：通过解析源数据库在线日志或归档日志获得数据的增删改变化。

## 二、Flume

#### 安装telnet
```
yum install telnet
telnet localhost 44444
```

#### 1.  sink为logger

```
bin/flume-ng agent --conf conf --conf-file example.conf --name a1 -Dflume.root.logger=INFO,console
```

#### 2. hdfs的sink
将sink type=hdfs的注释去掉：
 ```shell
a1.sinks.k1.type = hdfs
a1.sinks.k1.hdfs.path = hdfs:/flume
a1.sinks.k1.hdfs.filePrefix = events
a1.sinks.k1.hdfs.round = true
a1.sinks.k1.hdfs.roundValue = 10
a1.sinks.k1.hdfs.roundUnit = minute
a1.sinks.k1.hdfs.roundInterval = 60 
a1.sinks.k1.hdfs.fileType = DataStream
```

```
bin/flume-ng agent --conf conf --conf-file example.conf --name a1 -Dflume.root.logger=INFO,console
```
#### 3. 在source上增加拦截器
```shell
a1.sources.r1.interceptors = i1
a1.sources.r1.interceptors.i1.type =regex_filter
a1.sources.r1.interceptors.i1.regex =^[0-9]*$
a1.sources.r1.interceptors.i1.excludeEvents =true
```
#### 4. http header
文件：`header_test.conf`


master:
`curl -X POST -d '[{"headers" : {"timestamp" : "434324343","host" : "random_host.example.com"},"body" : "random_body"}]' master:9989`
```shell
Event: { headers:{host=random_host.example.com, timestamp=434324343} body: 72 61 6E 64 6F 6D 5F 62 6F 64 79                random_body }
```


slave3:
`curl -X POST -d '[{"headers" : {"timestamp" : "1","host" : "badou.com"},"body" : "badou"}]' master:9989`
```shell
Event: { headers:{host=badou.com, timestamp=1} body: 62 61 64 6F 75                                  badou }
```
#### 4. agent串行
master:
`bin/flume-ng agent --conf conf --conf-file conf/push.conf --name a1 -Dflume.root.logger=INFO,console`
slave3:
`bin/flume-ng agent --conf conf --conf-file conf/pull.conf --name a2 -Dflume.root.logger=INFO,console`


## 作业hive sink
不管哪个source，最终写入hive，[hive sink参考](http://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html#hive-sink)
```shell
a1.channels = c1
a1.channels.c1.type = memory
a1.sinks = k1
a1.sinks.k1.type = hive
a1.sinks.k1.channel = c1
a1.sinks.k1.hive.metastore = thrift://127.0.0.1:9083
a1.sinks.k1.hive.database = logsdb
a1.sinks.k1.hive.table = weblogs
a1.sinks.k1.hive.partition = asia,%{country},%y-%m-%d-%H-%M
a1.sinks.k1.useLocalTimeStamp = false
a1.sinks.k1.round = true
a1.sinks.k1.roundValue = 10
a1.sinks.k1.roundUnit = minute
a1.sinks.k1.serializer = DELIMITED
a1.sinks.k1.serializer.delimiter = "\t"
a1.sinks.k1.serializer.serdeSeparator = '\t'
a1.sinks.k1.serializer.fieldnames =id,,msg
```
### 5. flume->kafka
#### 1. 三个节点启动zookeeper
```shell
# 启动zk
./bin/zkServer.sh start
# 查看zk状态
./bin/zkServer.sh status
```
#### 2. 启动kafka
```
./bin/kafka-server-start.sh config/server.properties
```
#### 3. 启动flume
```shell
bin/flume-ng agent --conf conf --conf-file conf/flume_kafka.conf --name a1 -Dflume.root.logger=INFO,console
```
#### 4. 执行python脚本
```shell
python flume_data_write.py
```
可以先用echo传入部分数据先调试
```shell
echo "a">>  flume_exec_test.txt 
```

![e15ae03a5d0250a5bd5e2b3a50644c9e.png](en-resource://database/4298:0)
